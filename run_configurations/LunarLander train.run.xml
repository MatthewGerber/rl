<component name="ProjectRunConfigurationManager">
  <configuration default="false" name="LunarLander train" type="PythonConfigurationType" factoryName="Python" folderName="OpenAI Gym">
    <module name="rlai" />
    <option name="INTERPRETER_OPTIONS" value="" />
    <option name="PARENT_ENVS" value="true" />
    <envs>
      <env name="PYTHONUNBUFFERED" value="1" />
    </envs>
    <option name="SDK_HOME" value="" />
    <option name="WORKING_DIRECTORY" value="$PROJECT_DIR$/src/rlai/runners" />
    <option name="IS_MODULE_SDK" value="true" />
    <option name="ADD_CONTENT_ROOTS" value="true" />
    <option name="ADD_SOURCE_ROOTS" value="true" />
    <EXTENSION ID="PythonCoverageRunConfigurationExtension" runner="coverage.py" />
    <option name="SCRIPT_NAME" value="$PROJECT_DIR$/src/rlai/runners/trainer.py" />
    <option name="PARAMETERS" value="--random-seed 12345 --agent rlai.core.agents.ActionValueMdpAgent --continuous-state-discretization-resolution 0.25 --gamma 1 --environment rlai.core.environments.openai_gym.Gym --gym-id LunarLander-v2 --render-every-nth-episode 10000 --video-directory ~/Desktop/lunarlander_videos --train-function rlai.gpi.temporal_difference.iteration.iterate_value_q_pi --mode Q_LEARNING --num-improvements 10000 --num-episodes-per-improvement 10 --epsilon 0.1 --T 1000 --q-S-A rlai.gpi.state_action_value.tabular.TabularStateActionValueEstimator --make-final-policy-greedy True --num-improvements-per-plot 100 --num-improvements-per-checkpoint 100 --checkpoint-path ~/Desktop/lunarlander_checkpoint.pickle --save-agent-path ~/Desktop/lunarlander_agent.pickle" />
    <option name="SHOW_COMMAND_LINE" value="false" />
    <option name="EMULATE_TERMINAL" value="false" />
    <option name="MODULE_MODE" value="false" />
    <option name="REDIRECT_INPUT" value="false" />
    <option name="INPUT_FILE" value="" />
    <method v="2" />
  </configuration>
</component>