<component name="ProjectRunConfigurationManager">
  <configuration default="false" name="Acrobat train" type="PythonConfigurationType" factoryName="Python" folderName="OpenAI Gym">
    <module name="rl" />
    <option name="INTERPRETER_OPTIONS" value="" />
    <option name="PARENT_ENVS" value="true" />
    <envs>
      <env name="PYTHONUNBUFFERED" value="1" />
    </envs>
    <option name="SDK_HOME" value="" />
    <option name="WORKING_DIRECTORY" value="$PROJECT_DIR$/src/rlai/runners" />
    <option name="IS_MODULE_SDK" value="true" />
    <option name="ADD_CONTENT_ROOTS" value="true" />
    <option name="ADD_SOURCE_ROOTS" value="true" />
    <EXTENSION ID="PythonCoverageRunConfigurationExtension" runner="coverage.py" />
    <option name="SCRIPT_NAME" value="$PROJECT_DIR$/src/rlai/runners/trainer.py" />
    <option name="PARAMETERS" value="--train --agent &quot;rlai.agents.mdp.StochasticMdpAgent&quot; --continuous-state-discretization-resolution 0.5 --gamma 0.9 --environment &quot;rlai.environments.openai_gym.Gym&quot; --gym-id &quot;Acrobot-v1&quot; --render-every-nth-episode 1000 --video-directory &quot;~/Desktop/acrobat_videos&quot; --train-function &quot;rlai.gpi.temporal_difference.iteration.iterate_value_q_pi&quot; --mode &quot;Q_LEARNING&quot; --num-improvements 10000 --num-episodes-per-improvement 10 --epsilon 0.05 --state-action-value-estimator rlai.value_estimation.tabular.TabularStateActionValueEstimator --make-final-policy-greedy True --num-improvements-per-plot 100 --num-improvements-per-checkpoint 1000 --checkpoint-path &quot;~/Desktop/acrobat_checkpoint.pickle&quot; --save-agent-path &quot;~/Desktop/acrobat_agent.pickle&quot;" />
    <option name="SHOW_COMMAND_LINE" value="false" />
    <option name="EMULATE_TERMINAL" value="false" />
    <option name="MODULE_MODE" value="false" />
    <option name="REDIRECT_INPUT" value="false" />
    <option name="INPUT_FILE" value="" />
    <method v="2" />
  </configuration>
</component>