# Chapter 13:  Policy Gradient Methods
### [rlai.policies.parameterized.ParameterizedPolicy](https://github.com/MatthewGerber/rlai/tree/master/src/rlai/policies/parameterized.py#L20)
```
Policy for use with policy gradient methods.
```
### [rlai.policies.parameterized.SoftMaxInActionPreferencesJaxPolicy](https://github.com/MatthewGerber/rlai/tree/master/src/rlai/policies/parameterized.py#L234)
```
Parameterized policy that implements a soft-max over action preferences. The policy gradient calculation is
    performed using the JAX library.
```
### [rlai.policies.parameterized.SoftMaxInActionPreferencesPolicy](https://github.com/MatthewGerber/rlai/tree/master/src/rlai/policies/parameterized.py#L39)
```
Parameterized policy that implements a soft-max over action preferences. The policy gradient calculation is coded up
    manually. See the `JaxSoftMaxInActionPreferencesPolicy` for a similar policy in which the gradient is calculated
    using the JAX library.
```
### [rlai.policy_gradient.monte_carlo.reinforce.improve](https://github.com/MatthewGerber/rlai/tree/master/src/rlai/policy_gradient/monte_carlo/reinforce.py#L11)
```
Perform Monte Carlo improvement of an agent's policy within an environment via the REINFORCE policy gradient method.
    This improvement function operates over rewards obtained at the end of episodes, so it is only appropriate for
    episodic tasks.

    :param agent: Agent containing target policy to be optimized.
    :param policy: Parameterized policy to be optimized.
    :param environment: Environment.
    :param num_episodes: Number of episodes to execute.
    :param update_upon_every_visit: True to update each state-action pair upon each visit within an episode, or False to
    update each state-action pair upon the first visit within an episode.
    :param alpha: Policy gradient step size.
    :param thread_manager: Thread manager. The current function (and the thread running it) will wait on this manager
    before starting each iteration. This provides a mechanism for pausing, resuming, and aborting training. Omit for no
    waiting.
    :param v_S: Baseline state-value estimator.
```
