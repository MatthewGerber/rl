{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "significant-frontier",
   "metadata": {},
   "source": [
    "Install\n",
    "  * brew install node\n",
    "  * jupyter labextension install @jupyter-widgets/jupyterlab-manager\n",
    "  * jupyter labextension install jupyter-matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "virtual-divorce",
   "metadata": {},
   "source": [
    "# Agent Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "informed-opening",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76d2bc6bd05845039b3ed68ac42a895b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value iteration 1:  Running temporal-difference evaluation of q_pi for 50 episode(s).\n",
      "Finished 2 of 50 episode(s).\n",
      "Finished 4 of 50 episode(s).\n",
      "Finished 6 of 50 episode(s).\n",
      "Finished 8 of 50 episode(s).\n",
      "Finished 10 of 50 episode(s).\n",
      "Finished 12 of 50 episode(s).\n",
      "Finished 14 of 50 episode(s).\n",
      "Finished 16 of 50 episode(s).\n",
      "Finished 18 of 50 episode(s).\n",
      "Finished 20 of 50 episode(s).\n",
      "Finished 22 of 50 episode(s).\n",
      "Finished 24 of 50 episode(s).\n",
      "Finished 26 of 50 episode(s).\n",
      "Finished 28 of 50 episode(s).\n",
      "Finished 30 of 50 episode(s).\n",
      "Finished 32 of 50 episode(s).\n",
      "Finished 34 of 50 episode(s).\n",
      "Finished 36 of 50 episode(s).\n",
      "Finished 38 of 50 episode(s).\n",
      "Finished 40 of 50 episode(s).\n",
      "Finished 42 of 50 episode(s).\n",
      "Finished 44 of 50 episode(s).\n",
      "Finished 46 of 50 episode(s).\n",
      "Finished 48 of 50 episode(s).\n",
      "Finished 50 of 50 episode(s).\n",
      "Value iteration 2:  Running temporal-difference evaluation of q_pi for 50 episode(s).\n",
      "Finished 2 of 50 episode(s).\n",
      "Finished 4 of 50 episode(s).\n"
     ]
    }
   ],
   "source": [
    "%matplotlib widget\n",
    "\n",
    "from rlai.agents.mdp import StochasticMdpAgent\n",
    "from rlai.gpi.temporal_difference.evaluation import Mode\n",
    "from rlai.value_estimation.tabular import TabularStateActionValueEstimator\n",
    "from rlai.gpi.temporal_difference.iteration import iterate_value_q_pi\n",
    "from numpy.random import RandomState\n",
    "from rlai.environments.gridworld import Gridworld\n",
    "import matplotlib.pyplot as plt\n",
    "from rlai.gpi.utils import plot_policy_iteration, update_policy_iteration_plot\n",
    "from matplotlib.animation import FuncAnimation\n",
    "from threading import Thread\n",
    "import traceback\n",
    "\n",
    "fig = plot_policy_iteration(\n",
    "    iteration_average_reward=[],\n",
    "    iteration_total_states=[],\n",
    "    iteration_num_states_improved=[],\n",
    "    elapsed_seconds_average_rewards={},\n",
    "    pdf=None\n",
    ")\n",
    "\n",
    "def animate(\n",
    "    i\n",
    "):\n",
    "    try:\n",
    "        update_policy_iteration_plot()\n",
    "    except Exception as ex:\n",
    "        with open(f'/Users/mvg0419/Desktop/log_{i}_exception.txt', 'w') as f:\n",
    "            f.write(f'{ex}')\n",
    "            traceback.print_exc(file=f)\n",
    "\n",
    "ani = FuncAnimation(\n",
    "    fig, \n",
    "    animate, \n",
    "    frames=1000, \n",
    "    interval=1000,\n",
    "    repeat=False\n",
    ")\n",
    "\n",
    "def train():\n",
    "    \n",
    "    random_state = RandomState(12345)\n",
    "\n",
    "    mdp_environment: Gridworld = Gridworld.example_4_1(random_state, None)\n",
    "\n",
    "    epsilon = 0.05\n",
    "\n",
    "    q_S_A = TabularStateActionValueEstimator(mdp_environment, epsilon, None)\n",
    "\n",
    "    mdp_agent = StochasticMdpAgent(\n",
    "        'test',\n",
    "        random_state,\n",
    "        q_S_A.get_initial_policy(),\n",
    "        1\n",
    "    )\n",
    "\n",
    "    iterate_value_q_pi(\n",
    "        agent=mdp_agent,\n",
    "        environment=mdp_environment,\n",
    "        num_improvements=1000000,\n",
    "        num_episodes_per_improvement=50,\n",
    "        num_updates_per_improvement=None,\n",
    "        alpha=0.1,\n",
    "        mode=Mode.SARSA,\n",
    "        n_steps=1,\n",
    "        epsilon=epsilon,\n",
    "        planning_environment=None,\n",
    "        make_final_policy_greedy=True,\n",
    "        q_S_A=q_S_A,\n",
    "        num_improvements_per_plot=10\n",
    "    )\n",
    "    \n",
    "train_t = Thread(target=train)\n",
    "train_t.start()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
